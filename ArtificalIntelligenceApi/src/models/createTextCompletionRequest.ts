/**
 * Artifical Intelligence APILib
 *
 * This file was automatically generated by APIMATIC v3.0 ( https://www.apimatic.io ).
 */

import {
  array,
  boolean,
  lazy,
  number,
  object,
  optional,
  Schema,
  string,
} from '../schema';
import { Metadata, metadataSchema } from './metadata';

export interface CreateTextCompletionRequest {
  /** The model that will complete your prompt. */
  model: string;
  /**
   * The prompt that you want Claude to complete.
   *  For proper response generation you will need to format your prompt using alternating
   * Human: and
   * Assistant: conversational turns.
   */
  prompt: string;
  /**
   * The maximum number of tokens to generate before stopping.
   *  Note that our models may stop before reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.
   */
  maxTokensToSample: number;
  /**
   * Sequences that will cause the model to stop generating.
   *  Our models stop on `
   * Human:`, and may include additional built-in stop sequences in the future. By providing the stop_sequences parameter, you may include additional strings that will cause the model to stop generating.
   */
  stopSequences?: string[];
  /**
   * Amount of randomness injected into the response.
   *  Defaults to 1.0. Ranges from 0.0 to 1.0. Use temperature closer to 0.0 for analytical / multiple choice, and closer to 1.0 for creative and generative tasks.
   *  Note that even with temperature of 0.0, the results will not be fully deterministic
   */
  temperature?: number;
  /**
   * Use nucleus sampling.
   *  In nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by top_p. You should either alter temperature or top_p, but not both.
   *  Recommended for advanced use cases only. You usually only need to use temperature
   */
  topP?: number;
  /**
   * Only sample from the top K options for each subsequent token.
   *  Used to remove `long tail` low probability responses. Learn more technical details here.
   *  Recommended for advanced use cases only. You usually only need to use temperature.
   */
  topK?: number;
  /** An object describing metadata about the request. */
  metadata?: Metadata;
  /** Whether to incrementally stream the response using server-sent events. */
  stream?: boolean;
}

export const createTextCompletionRequestSchema: Schema<CreateTextCompletionRequest> = object(
  {
    model: ['model', string()],
    prompt: ['prompt', string()],
    maxTokensToSample: ['max_tokens_to_sample', number()],
    stopSequences: ['stop_sequences', optional(array(string()))],
    temperature: ['temperature', optional(number())],
    topP: ['top_p', optional(number())],
    topK: ['top_k', optional(number())],
    metadata: ['metadata', optional(lazy(() => metadataSchema))],
    stream: ['stream', optional(boolean())],
  }
);

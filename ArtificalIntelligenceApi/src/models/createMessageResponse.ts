/**
 * Artifical Intelligence APILib
 *
 * This file was automatically generated by APIMATIC v3.0 ( https://www.apimatic.io ).
 */

import { array, lazy, nullable, object, Schema, string } from '../schema';
import {
  CreateMessageResponseContent,
  createMessageResponseContentSchema,
} from './containers/createMessageResponseContent';
import { Role1Enum, role1EnumSchema } from './role1Enum';
import { StopReasonEnum, stopReasonEnumSchema } from './stopReasonEnum';
import { Type14Enum, type14EnumSchema } from './type14Enum';
import { Usage, usageSchema } from './usage';

export interface CreateMessageResponse {
  /**
   * Unique object identifier.
   *  The format and length of IDs may change over time.
   */
  id: string;
  /** For Messages, this is always `message` */
  type: Type14Enum;
  /** Conversational role of the generated message. */
  role: Role1Enum;
  content: CreateMessageResponseContent[];
  /** The model that handled the request. */
  model: string;
  /**
   * The reason that we stopped.
   *  This may be one the following values:
   *  `end_turn`: the model reached a natural stopping point
   *  `max_tokens`: we exceeded the requested max_tokens or the model's maximum
   *  `stop_sequence`: one of your provided custom stop_sequences was generated
   *  `tool_use`: the model invoked one or more tools
   *  In non-streaming mode this value is always non-null. In streaming mode, it is null in the message_start event and non-null otherwise.
   */
  stopReason: StopReasonEnum | null;
  /**
   * Which custom stop sequence was generated, if any.
   *  This value will be a non-null string if one of your custom stop sequences was generated.
   */
  stopSequence: string | null;
  /**
   * Billing and rate-limit usage.
   *  Anthropic's API bills and rate-limits by token counts, as tokens represent the underlying cost to our systems.
   *  Under the hood, the API transforms requests into a format suitable for the model. The model's output then goes through a parsing stage before becoming an API response. As a result, the token counts in usage will not match one-to-one with the exact visible content of an API request or response.
   *  For example, output_tokens will be non-zero, even for an empty string response from Claude.
   */
  usage: Usage;
}

export const createMessageResponseSchema: Schema<CreateMessageResponse> = object(
  {
    id: ['id', string()],
    type: ['type', type14EnumSchema],
    role: ['role', role1EnumSchema],
    content: ['content', array(createMessageResponseContentSchema)],
    model: ['model', string()],
    stopReason: ['stop_reason', nullable(stopReasonEnumSchema)],
    stopSequence: ['stop_sequence', nullable(string())],
    usage: ['usage', lazy(() => usageSchema)],
  }
);
